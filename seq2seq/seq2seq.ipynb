{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting between instruments using a Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import librosa\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torchaudio processing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized(tensor):\n",
    "    # https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html\n",
    "    centered = tensor - tensor.mean()\n",
    "    normalized = tensor / tensor.abs().max()\n",
    "    return normalized\n",
    "\n",
    "_mu_encoder = torchaudio.transforms.MuLawEncoding()\n",
    "_mu_decoder = torchaudio.transforms.MuLawDecoding()\n",
    "\n",
    "def mu_law_encode(waveform):\n",
    "    return _mu_encoder(normalized(waveform))\n",
    "\n",
    "def mu_law_decode(waveform):\n",
    "    return _mu_decoder(normalized(waveform))\n",
    "\n",
    "def load_audio(path):\n",
    "    \"\"\" Load .wav file to Mu law encoding tensor \"\"\"\n",
    "    waveform, sample_rate = torchaudio.load(path)\n",
    "    return mu_law_encode(waveform), sample_rate\n",
    "\n",
    "def save_audio(path, data, sample_rate):\n",
    "    \"\"\" Save Mu law encoding tensor to .wav file \"\"\"\n",
    "    waveform = mu_law_decode(data)\n",
    "    torchaudio.save(path, data, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting wav files to tensors and saving them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Path.cwd().joinpath(\"dataset\")\n",
    "train_wav = dataset.joinpath(\"train_wav\")\n",
    "train_tensors = dataset.joinpath(\"train_tensors\")\n",
    "test_wav = dataset.joinpath(\"test_wav\")\n",
    "test_tensors = dataset.joinpath(\"test_tensors\")\n",
    "\n",
    "for instrument in train_wav.iterdir():\n",
    "    if (instrument.name != \".DS_Store\"):\n",
    "        for note in train_wav.joinpath(instrument.name).iterdir():\n",
    "            if (note.name != \".DS_Store\"):\n",
    "                wav_tensor = load_audio(train_wav.joinpath(instrument.name, note.name))\n",
    "                wav_tensor = (wav_tensor[0][0] + wav_tensor[0][1]) / 2\n",
    "                torch.save(wav_tensor, train_tensors.joinpath(instrument.name, note.stem + \".pt\"))\n",
    "                \n",
    "for instrument in test_wav.iterdir():\n",
    "    if (instrument.name != \".DS_Store\"):\n",
    "        for note in test_wav.joinpath(instrument.name).iterdir():\n",
    "            if (note.name != \".DS_Store\"):\n",
    "                wav_tensor = load_audio(test_wav.joinpath(instrument.name, note.name))\n",
    "                wav_tensor = (wav_tensor[0][0] + wav_tensor[0][1]) / 2\n",
    "                torch.save(wav_tensor, test_tensors.joinpath(instrument.name, note.stem + \".pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in the tensors and preparing them for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train lengths: 45 45\n",
      "test lengths: 15 15\n"
     ]
    }
   ],
   "source": [
    "dataset = Path.cwd().joinpath(\"dataset\")\n",
    "\n",
    "#Change for model you are training\n",
    "train_input = dataset.joinpath(\"train_tensors\", \"75\")\n",
    "train_output = dataset.joinpath(\"train_tensors\", \"81\")\n",
    "test_input = dataset.joinpath(\"test_tensors\", \"75\")\n",
    "test_output = dataset.joinpath(\"test_tensors\", \"81\")\n",
    "\n",
    "train_input_wav = []\n",
    "train_output_wav = []\n",
    "test_input_wav = []\n",
    "test_output_wav = []\n",
    "\n",
    "#Take out DS_Stores if you are using a Mac\n",
    "for tensor in train_input.iterdir():\n",
    "    if tensor.name != \".DS_Store\":\n",
    "        train_input_wav.append(tensor)\n",
    "    \n",
    "for tensor in train_output.iterdir():\n",
    "    if tensor.name != \".DS_Store\":\n",
    "        train_output_wav.append(tensor)\n",
    "\n",
    "for tensor in test_input.iterdir():\n",
    "    if tensor.name != \".DS_Store\":\n",
    "        test_input_wav.append(tensor)\n",
    "    \n",
    "for tensor in test_output.iterdir():\n",
    "    if tensor.name != \".DS_Store\":\n",
    "        test_output_wav.append(tensor)\n",
    "        \n",
    "print(\"train lengths:\", len(train_input_wav), len(train_output_wav))\n",
    "print(\"test lengths:\", len(test_input_wav), len(test_output_wav))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Seq2Seq Dataset. Uses torchaudio + mu law to process wav files.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_wavs, output_wavs, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_wavs: list of input wavs from 1st instrument\n",
    "            output_wavs: list of output wavs from 2nd instrument\n",
    "        \"\"\"\n",
    "        self.input_wavs = input_wavs\n",
    "        self.output_wavs = output_wavs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_wavs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = torch.load(self.input_wavs[index])\n",
    "        output = torch.load(self.output_wavs[index])\n",
    "        return input, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data loaders using Seq2SeqDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 45\n",
      "Test set size: 15\n"
     ]
    }
   ],
   "source": [
    "train_set = Seq2SeqDataset(train_input_wav, train_output_wav)\n",
    "test_set = Seq2SeqDataset(test_input_wav, test_output_wav)\n",
    "print(\"Train set size: \" + str(len(train_set)))\n",
    "print(\"Test set size: \" + str(len(test_set)))\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {} #needed for using datasets on gpu\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 8, shuffle = True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 8, shuffle = True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "tensor([[122, 122, 125,  ...,  16,  17,  19],\n",
      "        [121, 121, 124,  ..., 103, 124, 142],\n",
      "        [122, 122, 125,  ..., 204, 186,  70],\n",
      "        ...,\n",
      "        [122, 122, 125,  ...,  10,  11,  14],\n",
      "        [121, 121, 124,  ...,   7,   7,   7],\n",
      "        [122, 122, 125,  ...,  70,  86, 134]]) tensor([[124, 124, 126,  ..., 219, 219, 219],\n",
      "        [124, 124, 126,  ...,  67,  63,  58],\n",
      "        [124, 124, 126,  ..., 146, 145, 143],\n",
      "        ...,\n",
      "        [124, 124, 126,  ..., 143, 223, 232],\n",
      "        [124, 124, 126,  ..., 101,  84,  37],\n",
      "        [123, 123, 125,  ..., 213, 212, 211]])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "tensor([[121, 121, 124,  ..., 237, 236, 235],\n",
      "        [122, 122, 125,  ..., 204, 210, 214],\n",
      "        [121, 121, 124,  ...,  40,  42,  43],\n",
      "        ...,\n",
      "        [122, 122, 125,  ..., 229, 218, 197],\n",
      "        [122, 122, 125,  ...,  25,  20,  17],\n",
      "        [121, 121, 124,  ..., 219, 218, 216]]) tensor([[124, 124, 126,  ..., 197, 188, 168],\n",
      "        [124, 124, 126,  ..., 113, 112, 111],\n",
      "        [124, 124, 126,  ...,  75,  70,  67],\n",
      "        ...,\n",
      "        [124, 124, 126,  ...,  45,  42,  42],\n",
      "        [124, 124, 126,  ...,  73,  57,  48],\n",
      "        [123, 123, 125,  ..., 198, 140, 132]])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "tensor([[122, 122, 125,  ..., 231, 229, 227],\n",
      "        [122, 122, 125,  ..., 232, 232, 231],\n",
      "        [122, 122, 125,  ...,  35,  26,  20],\n",
      "        ...,\n",
      "        [121, 121, 124,  ..., 242, 242, 243],\n",
      "        [121, 121, 124,  ..., 206, 202, 198],\n",
      "        [122, 122, 125,  ..., 225, 221, 212]]) tensor([[123, 123, 125,  ..., 152, 150, 147],\n",
      "        [123, 123, 125,  ..., 128, 128, 128],\n",
      "        [124, 124, 126,  ..., 139, 137, 135],\n",
      "        ...,\n",
      "        [123, 123, 125,  ..., 222, 222, 145],\n",
      "        [123, 123, 125,  ..., 129, 128, 128],\n",
      "        [124, 124, 126,  ..., 139, 138, 158]])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "tensor([[121, 121, 124,  ...,  10,  11,  11],\n",
      "        [122, 122, 125,  ...,   7,   6,   6],\n",
      "        [122, 122, 125,  ...,  12,  11,  10],\n",
      "        ...,\n",
      "        [122, 122, 125,  ..., 251, 252, 251],\n",
      "        [121, 121, 124,  ..., 236, 235, 233],\n",
      "        [121, 121, 124,  ..., 234, 235, 235]]) tensor([[124, 124, 126,  ...,  68, 108, 123],\n",
      "        [123, 123, 125,  ..., 205, 204, 203],\n",
      "        [124, 124, 126,  ..., 207, 154, 135],\n",
      "        ...,\n",
      "        [124, 124, 126,  ...,  38,  36,  35],\n",
      "        [124, 124, 126,  ..., 148, 147, 147],\n",
      "        [124, 124, 126,  ..., 227, 219, 220]])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "tensor([[122, 122, 125,  ..., 231, 230, 226],\n",
      "        [122, 122, 125,  ...,  17,  16,  16],\n",
      "        [122, 122, 125,  ...,  66,  76,  84],\n",
      "        ...,\n",
      "        [122, 122, 125,  ...,  15,  14,  13],\n",
      "        [121, 121, 124,  ..., 233, 234, 235],\n",
      "        [121, 121, 124,  ...,  26,  26,  26]]) tensor([[123, 123, 125,  ..., 131, 131, 131],\n",
      "        [124, 124, 126,  ..., 114, 114, 114],\n",
      "        [124, 124, 126,  ...,  42,  41,  41],\n",
      "        ...,\n",
      "        [124, 124, 126,  ...,  40,  37,  34],\n",
      "        [124, 124, 126,  ..., 144, 147, 145],\n",
      "        [123, 123, 125,  ..., 216, 213, 208]])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "torch.Size([22208])\n",
      "tensor([[121, 121, 124,  ..., 221, 221, 221],\n",
      "        [122, 122, 125,  ..., 226, 226, 225],\n",
      "        [121, 121, 124,  ..., 239, 239, 239],\n",
      "        [121, 121, 124,  ...,  17,  18,  19],\n",
      "        [121, 121, 124,  ...,  18,  18,  18]]) tensor([[123, 123, 125,  ..., 133, 132, 133],\n",
      "        [124, 124, 126,  ..., 136, 134, 131],\n",
      "        [123, 123, 125,  ..., 141, 140, 140],\n",
      "        [124, 124, 126,  ..., 125, 124, 123],\n",
      "        [124, 124, 126,  ..., 137, 135, 132]])\n"
     ]
    }
   ],
   "source": [
    "for data, target in train_loader:\n",
    "    print(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
