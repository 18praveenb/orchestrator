{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting between instruments using a Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import librosa\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torchaudio processing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized(tensor):\n",
    "    # https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html\n",
    "    centered = tensor - tensor.mean()\n",
    "    normalized = tensor / tensor.abs().max()\n",
    "    return normalized\n",
    "\n",
    "_mu_encoder = torchaudio.transforms.MuLawEncoding()\n",
    "_mu_decoder = torchaudio.transforms.MuLawDecoding()\n",
    "\n",
    "def mu_law_encode(waveform):\n",
    "    return _mu_encoder(normalized(waveform))\n",
    "\n",
    "def mu_law_decode(waveform):\n",
    "    return _mu_decoder(normalized(waveform))\n",
    "\n",
    "def load_audio(path):\n",
    "    \"\"\" Load .wav file to Mu law encoding tensor \"\"\"\n",
    "    waveform, sample_rate = torchaudio.load(path)\n",
    "    return mu_law_encode(waveform), sample_rate\n",
    "\n",
    "def save_audio(path, data, sample_rate):\n",
    "    \"\"\" Save Mu law encoding tensor to .wav file \"\"\"\n",
    "    waveform = mu_law_decode(data)\n",
    "    torchaudio.save(path, data, sample_rate)\n",
    "    \n",
    "SOS_token = 0\n",
    "EOS_token = -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting wav files to tensors and saving them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Path.cwd().joinpath(\"dataset\")\n",
    "train_wav = dataset.joinpath(\"train_wav\")\n",
    "train_tensors = dataset.joinpath(\"train_tensors\")\n",
    "test_wav = dataset.joinpath(\"test_wav\")\n",
    "test_tensors = dataset.joinpath(\"test_tensors\")\n",
    "\n",
    "for instrument in train_wav.iterdir():\n",
    "    if (instrument.name != \".DS_Store\"):\n",
    "        for note in train_wav.joinpath(instrument.name).iterdir():\n",
    "            if (note.name != \".DS_Store\"):\n",
    "                wav_tensor = load_audio(train_wav.joinpath(instrument.name, note.name))\n",
    "                wav_tensor = (wav_tensor[0][0] + wav_tensor[0][1]) / 2\n",
    "                torch.save(wav_tensor, train_tensors.joinpath(instrument.name, note.stem + \".pt\"))\n",
    "                \n",
    "for instrument in test_wav.iterdir():\n",
    "    if (instrument.name != \".DS_Store\"):\n",
    "        for note in test_wav.joinpath(instrument.name).iterdir():\n",
    "            if (note.name != \".DS_Store\"):\n",
    "                wav_tensor = load_audio(test_wav.joinpath(instrument.name, note.name))\n",
    "                wav_tensor = (wav_tensor[0][0] + wav_tensor[0][1]) / 2\n",
    "                torch.save(wav_tensor, test_tensors.joinpath(instrument.name, note.stem + \".pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in the tensors and preparing them for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train lengths: 45 45\n",
      "test lengths: 15 15\n"
     ]
    }
   ],
   "source": [
    "dataset = Path.cwd().joinpath(\"dataset\")\n",
    "\n",
    "#Change for model you are training\n",
    "train_input = dataset.joinpath(\"train_tensors\", \"75\")\n",
    "train_output = dataset.joinpath(\"train_tensors\", \"81\")\n",
    "test_input = dataset.joinpath(\"test_tensors\", \"75\")\n",
    "test_output = dataset.joinpath(\"test_tensors\", \"81\")\n",
    "\n",
    "train_input_wav = []\n",
    "train_output_wav = []\n",
    "test_input_wav = []\n",
    "test_output_wav = []\n",
    "\n",
    "#Take out DS_Stores if you are using a Mac\n",
    "for tensor in train_input.iterdir():\n",
    "    if tensor.name != \".DS_Store\":\n",
    "        train_input_wav.append(tensor)\n",
    "    \n",
    "for tensor in train_output.iterdir():\n",
    "    if tensor.name != \".DS_Store\":\n",
    "        train_output_wav.append(tensor)\n",
    "\n",
    "for tensor in test_input.iterdir():\n",
    "    if tensor.name != \".DS_Store\":\n",
    "        test_input_wav.append(tensor)\n",
    "    \n",
    "for tensor in test_output.iterdir():\n",
    "    if tensor.name != \".DS_Store\":\n",
    "        test_output_wav.append(tensor)\n",
    "        \n",
    "print(\"train lengths:\", len(train_input_wav), len(train_output_wav))\n",
    "print(\"test lengths:\", len(test_input_wav), len(test_output_wav))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Seq2Seq Dataset. Uses torchaudio + mu law to process wav files.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_wavs, output_wavs, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_wavs: list of input wavs from 1st instrument\n",
    "            output_wavs: list of output wavs from 2nd instrument\n",
    "        \"\"\"\n",
    "        self.input_wavs = input_wavs\n",
    "        self.output_wavs = output_wavs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_wavs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = torch.load(self.input_wavs[index])\n",
    "        output = torch.load(self.output_wavs[index])\n",
    "        return input[::2000], output[::2000]\n",
    "#         return torch.cat((input[::10], torch.tensor([EOS_token])), 0), torch.cat((output[::10], torch.tensor([EOS_token])), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data loaders using Seq2SeqDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 45\n",
      "Test set size: 15\n"
     ]
    }
   ],
   "source": [
    "train_set = Seq2SeqDataset(train_input_wav, train_output_wav)\n",
    "test_set = Seq2SeqDataset(test_input_wav, test_output_wav)\n",
    "print(\"Train set size: \" + str(len(train_set)))\n",
    "print(\"Test set size: \" + str(len(test_set)))\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {} #needed for using datasets on gpu\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 8, shuffle = True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 8, shuffle = True, **kwargs)\n",
    "\n",
    "seq_length = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n",
      "torch.Size([12])\n",
      "torch.Size([12])\n",
      "torch.Size([12])\n",
      "torch.Size([12])\n",
      "torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "for data, target in train_loader:\n",
    "    print(data[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.gru = nn.GRU(1, 1, num_layers = 1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.gru = nn.GRU(1, 1, num_layers = 1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN().to(device)\n",
    "decoder = DecoderRNN().to(device)\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr = 0.01, weight_decay = 0.0001)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr = 0.01, weight_decay = 0.0001)\n",
    "encoder_scheduler = optim.lr_scheduler.StepLR(encoder_optimizer, step_size = 20, gamma = 0.1)\n",
    "decoder_scheduler = optim.lr_scheduler.StepLR(decoder_optimizer, step_size = 20, gamma = 0.1)\n",
    "criterion = nn.MSELoss()\n",
    "teacher_forcing_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, epoch):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        \n",
    "        data = torch.unsqueeze(data, 2)\n",
    "        target = torch.unsqueeze(target, 2)\n",
    "        data = data.permute(1, 0, 2).float()\n",
    "        target = target.permute(1, 0, 2).float()\n",
    "        data_len = data.size(0)\n",
    "        target_len = target.size(0)\n",
    "        batch_len = data.size(1)\n",
    "        \n",
    "        encoder_hidden = torch.zeros([1, batch_len, 1], dtype=torch.float, device=device)\n",
    "        encoder_output, encoder_hidden = encoder(data, encoder_hidden)\n",
    "\n",
    "        decoder_input = torch.zeros([1, batch_len, 1], dtype=torch.float, device=device)\n",
    "        for i in range(target.size(1)):\n",
    "            decoder_input[0][i] = torch.tensor([[SOS_token]], dtype=torch.float, device=device)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden\n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output.view(-1, 1), target[0])\n",
    "            for i in range(target_len - 1):\n",
    "                decoder_output, decoder_hidden = decoder(target[i].unsqueeze(0), decoder_hidden)\n",
    "                loss += criterion(decoder_output.view(-1, 1), target[i + 1])\n",
    "        else:\n",
    "            for i in range(target_len):\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                loss += criterion(decoder_output.view(-1, 1), target[i])\n",
    "                decoder_input = decoder_output\n",
    "        \n",
    "#         loss /= seq_length\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        if batch_idx % log_interval == 0: #print training stats\n",
    "            print('Train Epoch: {} Batch: {}\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(encoder, decoder, epoch):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    for data, target in test_loader:\n",
    "        loss = 0\n",
    "        \n",
    "        data = torch.unsqueeze(data, 2)\n",
    "        target = torch.unsqueeze(target, 2)\n",
    "        data = data.permute(1, 0, 2).float()\n",
    "        target = target.permute(1, 0, 2).float()\n",
    "        data_len = data.size(0)\n",
    "        target_len = target.size(0)\n",
    "        batch_len = data.size(1)\n",
    "        \n",
    "        encoder_hidden = torch.zeros([1, batch_len, 1], dtype=torch.float, device=device)\n",
    "        encoder_output, encoder_hidden = encoder(data, encoder_hidden)\n",
    "\n",
    "        decoder_input = torch.zeros([1, batch_len, 1], dtype=torch.float, device=device)\n",
    "        for i in range(target.size(1)):\n",
    "            decoder_input[0][i] = torch.tensor([[SOS_token]], dtype=torch.float, device=device)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        for i in range(target_len):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output.view(-1, 1), target[i])\n",
    "            decoder_input = decoder_output\n",
    "        \n",
    "#         loss /= seq_length\n",
    "    print('\\nTest set: Loss: {}\\n'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Batch: 0\tLoss: 247325.062500\n",
      "Train Epoch: 1 Batch: 2\tLoss: 244293.078125\n",
      "Train Epoch: 1 Batch: 4\tLoss: 217919.953125\n",
      "\n",
      "Test set: Loss: 234219.234375\n",
      "\n",
      "Train Epoch: 2 Batch: 0\tLoss: 218389.468750\n",
      "Train Epoch: 2 Batch: 2\tLoss: 240846.437500\n",
      "Train Epoch: 2 Batch: 4\tLoss: 235160.812500\n",
      "\n",
      "Test set: Loss: 232446.125\n",
      "\n",
      "Train Epoch: 3 Batch: 0\tLoss: 241247.468750\n",
      "Train Epoch: 3 Batch: 2\tLoss: 236456.515625\n",
      "Train Epoch: 3 Batch: 4\tLoss: 246191.281250\n",
      "\n",
      "Test set: Loss: 239718.015625\n",
      "\n",
      "Train Epoch: 4 Batch: 0\tLoss: 248954.828125\n",
      "Train Epoch: 4 Batch: 2\tLoss: 222878.421875\n",
      "Train Epoch: 4 Batch: 4\tLoss: 243919.171875\n",
      "\n",
      "Test set: Loss: 229102.046875\n",
      "\n",
      "Train Epoch: 5 Batch: 0\tLoss: 221610.109375\n",
      "Train Epoch: 5 Batch: 2\tLoss: 211299.468750\n",
      "Train Epoch: 5 Batch: 4\tLoss: 264082.125000\n",
      "\n",
      "Test set: Loss: 237163.6875\n",
      "\n",
      "Train Epoch: 6 Batch: 0\tLoss: 208614.234375\n",
      "Train Epoch: 6 Batch: 2\tLoss: 223187.531250\n",
      "Train Epoch: 6 Batch: 4\tLoss: 248786.750000\n",
      "\n",
      "Test set: Loss: 238508.0\n",
      "\n",
      "Train Epoch: 7 Batch: 0\tLoss: 246133.640625\n",
      "Train Epoch: 7 Batch: 2\tLoss: 208384.546875\n",
      "Train Epoch: 7 Batch: 4\tLoss: 254594.812500\n",
      "\n",
      "Test set: Loss: 202552.3125\n",
      "\n",
      "Train Epoch: 8 Batch: 0\tLoss: 207658.343750\n",
      "Train Epoch: 8 Batch: 2\tLoss: 234069.093750\n",
      "Train Epoch: 8 Batch: 4\tLoss: 240979.421875\n",
      "\n",
      "Test set: Loss: 222408.09375\n",
      "\n",
      "Train Epoch: 9 Batch: 0\tLoss: 228912.562500\n",
      "Train Epoch: 9 Batch: 2\tLoss: 267641.687500\n",
      "Train Epoch: 9 Batch: 4\tLoss: 247422.796875\n",
      "\n",
      "Test set: Loss: 245555.203125\n",
      "\n",
      "Train Epoch: 10 Batch: 0\tLoss: 258445.000000\n",
      "Train Epoch: 10 Batch: 2\tLoss: 210087.468750\n",
      "Train Epoch: 10 Batch: 4\tLoss: 245477.750000\n",
      "\n",
      "Test set: Loss: 249155.140625\n",
      "\n",
      "Train Epoch: 11 Batch: 0\tLoss: 242098.609375\n",
      "Train Epoch: 11 Batch: 2\tLoss: 240184.437500\n",
      "Train Epoch: 11 Batch: 4\tLoss: 213739.312500\n",
      "\n",
      "Test set: Loss: 239948.28125\n",
      "\n",
      "Train Epoch: 12 Batch: 0\tLoss: 238876.781250\n",
      "Train Epoch: 12 Batch: 2\tLoss: 232780.390625\n",
      "Train Epoch: 12 Batch: 4\tLoss: 231861.921875\n",
      "\n",
      "Test set: Loss: 220800.71875\n",
      "\n",
      "Train Epoch: 13 Batch: 0\tLoss: 242540.093750\n",
      "Train Epoch: 13 Batch: 2\tLoss: 228784.593750\n",
      "Train Epoch: 13 Batch: 4\tLoss: 235985.281250\n",
      "\n",
      "Test set: Loss: 203758.375\n",
      "\n",
      "Train Epoch: 14 Batch: 0\tLoss: 254495.156250\n",
      "Train Epoch: 14 Batch: 2\tLoss: 214325.078125\n",
      "Train Epoch: 14 Batch: 4\tLoss: 229111.359375\n",
      "\n",
      "Test set: Loss: 224916.453125\n",
      "\n",
      "Train Epoch: 15 Batch: 0\tLoss: 249976.906250\n",
      "Train Epoch: 15 Batch: 2\tLoss: 221958.968750\n",
      "Train Epoch: 15 Batch: 4\tLoss: 244972.046875\n",
      "\n",
      "Test set: Loss: 255364.390625\n",
      "\n",
      "Train Epoch: 16 Batch: 0\tLoss: 233301.328125\n",
      "Train Epoch: 16 Batch: 2\tLoss: 241914.406250\n",
      "Train Epoch: 16 Batch: 4\tLoss: 238178.859375\n",
      "\n",
      "Test set: Loss: 242897.921875\n",
      "\n",
      "Train Epoch: 17 Batch: 0\tLoss: 229256.859375\n",
      "Train Epoch: 17 Batch: 2\tLoss: 244283.031250\n",
      "Train Epoch: 17 Batch: 4\tLoss: 216980.078125\n",
      "\n",
      "Test set: Loss: 237770.25\n",
      "\n",
      "Train Epoch: 18 Batch: 0\tLoss: 247744.046875\n",
      "Train Epoch: 18 Batch: 2\tLoss: 219983.468750\n",
      "Train Epoch: 18 Batch: 4\tLoss: 235485.812500\n",
      "\n",
      "Test set: Loss: 226902.5625\n",
      "\n",
      "Train Epoch: 19 Batch: 0\tLoss: 238469.578125\n",
      "Train Epoch: 19 Batch: 2\tLoss: 231540.453125\n",
      "Train Epoch: 19 Batch: 4\tLoss: 231148.656250\n",
      "\n",
      "Test set: Loss: 211306.625\n",
      "\n",
      "Train Epoch: 20 Batch: 0\tLoss: 235015.359375\n",
      "Train Epoch: 20 Batch: 2\tLoss: 239186.734375\n",
      "Train Epoch: 20 Batch: 4\tLoss: 226624.765625\n",
      "\n",
      "Test set: Loss: 222272.78125\n",
      "\n",
      "Train Epoch: 21 Batch: 0\tLoss: 238922.140625\n",
      "Train Epoch: 21 Batch: 2\tLoss: 245485.906250\n",
      "Train Epoch: 21 Batch: 4\tLoss: 250452.734375\n",
      "\n",
      "Test set: Loss: 233002.03125\n",
      "\n",
      "Train Epoch: 22 Batch: 0\tLoss: 227431.968750\n",
      "Train Epoch: 22 Batch: 2\tLoss: 253818.781250\n",
      "Train Epoch: 22 Batch: 4\tLoss: 220986.562500\n",
      "\n",
      "Test set: Loss: 243516.546875\n",
      "\n",
      "Train Epoch: 23 Batch: 0\tLoss: 254311.046875\n",
      "Train Epoch: 23 Batch: 2\tLoss: 224643.296875\n",
      "Train Epoch: 23 Batch: 4\tLoss: 231736.406250\n",
      "\n",
      "Test set: Loss: 229125.984375\n",
      "\n",
      "Train Epoch: 24 Batch: 0\tLoss: 234473.781250\n",
      "Train Epoch: 24 Batch: 2\tLoss: 249256.812500\n",
      "Train Epoch: 24 Batch: 4\tLoss: 227867.203125\n",
      "\n",
      "Test set: Loss: 236463.515625\n",
      "\n",
      "Train Epoch: 25 Batch: 0\tLoss: 232794.890625\n",
      "Train Epoch: 25 Batch: 2\tLoss: 245369.671875\n",
      "Train Epoch: 25 Batch: 4\tLoss: 225299.109375\n",
      "\n",
      "Test set: Loss: 217139.328125\n",
      "\n",
      "Train Epoch: 26 Batch: 0\tLoss: 214922.875000\n",
      "Train Epoch: 26 Batch: 2\tLoss: 228255.593750\n",
      "Train Epoch: 26 Batch: 4\tLoss: 257337.390625\n",
      "\n",
      "Test set: Loss: 234075.359375\n",
      "\n",
      "Train Epoch: 27 Batch: 0\tLoss: 225841.906250\n",
      "Train Epoch: 27 Batch: 2\tLoss: 229734.921875\n",
      "Train Epoch: 27 Batch: 4\tLoss: 217585.328125\n",
      "\n",
      "Test set: Loss: 208849.765625\n",
      "\n",
      "Train Epoch: 28 Batch: 0\tLoss: 235394.546875\n",
      "Train Epoch: 28 Batch: 2\tLoss: 217850.546875\n",
      "Train Epoch: 28 Batch: 4\tLoss: 232514.453125\n",
      "\n",
      "Test set: Loss: 238173.265625\n",
      "\n",
      "Train Epoch: 29 Batch: 0\tLoss: 263116.843750\n",
      "Train Epoch: 29 Batch: 2\tLoss: 240523.187500\n",
      "Train Epoch: 29 Batch: 4\tLoss: 236276.515625\n",
      "\n",
      "Test set: Loss: 243669.0\n",
      "\n",
      "Train Epoch: 30 Batch: 0\tLoss: 228914.734375\n",
      "Train Epoch: 30 Batch: 2\tLoss: 251410.125000\n",
      "Train Epoch: 30 Batch: 4\tLoss: 219867.687500\n",
      "\n",
      "Test set: Loss: 229934.25\n",
      "\n",
      "First round of training complete. Setting learn rate to 0.001.\n",
      "Train Epoch: 31 Batch: 0\tLoss: 225018.109375\n",
      "Train Epoch: 31 Batch: 2\tLoss: 232456.625000\n",
      "Train Epoch: 31 Batch: 4\tLoss: 213849.375000\n",
      "\n",
      "Test set: Loss: 242086.34375\n",
      "\n",
      "Train Epoch: 32 Batch: 0\tLoss: 246504.406250\n",
      "Train Epoch: 32 Batch: 2\tLoss: 219065.390625\n",
      "Train Epoch: 32 Batch: 4\tLoss: 243456.375000\n",
      "\n",
      "Test set: Loss: 244335.21875\n",
      "\n",
      "Train Epoch: 33 Batch: 0\tLoss: 219630.828125\n",
      "Train Epoch: 33 Batch: 2\tLoss: 236752.718750\n",
      "Train Epoch: 33 Batch: 4\tLoss: 262873.406250\n",
      "\n",
      "Test set: Loss: 224103.375\n",
      "\n",
      "Train Epoch: 34 Batch: 0\tLoss: 231979.500000\n",
      "Train Epoch: 34 Batch: 2\tLoss: 228107.031250\n",
      "Train Epoch: 34 Batch: 4\tLoss: 200411.062500\n",
      "\n",
      "Test set: Loss: 235770.109375\n",
      "\n",
      "Train Epoch: 35 Batch: 0\tLoss: 245772.593750\n",
      "Train Epoch: 35 Batch: 2\tLoss: 239756.281250\n",
      "Train Epoch: 35 Batch: 4\tLoss: 241108.484375\n",
      "\n",
      "Test set: Loss: 233166.578125\n",
      "\n",
      "Train Epoch: 36 Batch: 0\tLoss: 233640.578125\n",
      "Train Epoch: 36 Batch: 2\tLoss: 240436.640625\n",
      "Train Epoch: 36 Batch: 4\tLoss: 240039.890625\n",
      "\n",
      "Test set: Loss: 223416.953125\n",
      "\n",
      "Train Epoch: 37 Batch: 0\tLoss: 221776.968750\n",
      "Train Epoch: 37 Batch: 2\tLoss: 215587.250000\n",
      "Train Epoch: 37 Batch: 4\tLoss: 244580.812500\n",
      "\n",
      "Test set: Loss: 225559.03125\n",
      "\n",
      "Train Epoch: 38 Batch: 0\tLoss: 233907.343750\n",
      "Train Epoch: 38 Batch: 2\tLoss: 230876.218750\n",
      "Train Epoch: 38 Batch: 4\tLoss: 255373.781250\n",
      "\n",
      "Test set: Loss: 236962.75\n",
      "\n",
      "Train Epoch: 39 Batch: 0\tLoss: 241417.765625\n",
      "Train Epoch: 39 Batch: 2\tLoss: 251326.000000\n",
      "Train Epoch: 39 Batch: 4\tLoss: 214887.265625\n",
      "\n",
      "Test set: Loss: 235206.5625\n",
      "\n",
      "Train Epoch: 40 Batch: 0\tLoss: 216121.125000\n",
      "Train Epoch: 40 Batch: 2\tLoss: 235050.046875\n",
      "Train Epoch: 40 Batch: 4\tLoss: 227110.156250\n",
      "\n",
      "Test set: Loss: 237293.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_interval = 2\n",
    "for epoch in range(1, 41):\n",
    "    if epoch == 31:\n",
    "        print(\"First round of training complete. Setting learn rate to 0.001.\")\n",
    "    encoder_scheduler.step()\n",
    "    decoder_scheduler.step()\n",
    "    train(encoder, decoder, epoch)\n",
    "    test(encoder, decoder, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
