{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a Pytorch Dataset for the Music Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import librosa\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized(tensor):\n",
    "    # https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html\n",
    "    centered = tensor - tensor.mean()\n",
    "    normalized = tensor / tensor.abs().max()\n",
    "    return normalized\n",
    "\n",
    "_mu_encoder = torchaudio.transforms.MuLawEncoding()\n",
    "_mu_decoder = torchaudio.transforms.MuLawDecoding()\n",
    "\n",
    "def mu_law_encode(waveform):\n",
    "    return _mu_encoder(normalized(waveform))\n",
    "\n",
    "def mu_law_decode(waveform):\n",
    "    return _mu_decoder(normalized(waveform))\n",
    "\n",
    "def load_audio(path):\n",
    "    \"\"\" Load .wav file to Mu law encoding tensor \"\"\"\n",
    "    waveform, sample_rate = torchaudio.load(path)\n",
    "    return mu_law_encode(waveform), sample_rate\n",
    "\n",
    "def save_audio(path, data, sample_rate):\n",
    "    \"\"\" Save Mu law encoding tensor to .wav file \"\"\"\n",
    "    waveform = mu_law_decode(data)\n",
    "    torchaudio.save(path, data, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Path.cwd().parent.joinpath(\"musicnet\", \"music_classification_data\")\n",
    "\n",
    "train_input = dataset.joinpath(\"train_input\")\n",
    "test_input = dataset.joinpath(\"test_input\")\n",
    "\n",
    "train_output = dataset.joinpath(\"train_output\")\n",
    "test_output = dataset.joinpath(\"test_output\")\n",
    "\n",
    "# Uncomment the code below if needed for your machine\n",
    "train_input.remove(\".DS_Store\")\n",
    "test_input.remove(\".DS_Store\")\n",
    "train_output.remove(\".DS_Store\")\n",
    "test_output.remove(\".DS_Store\")\n",
    "\n",
    "print(\"train labels:\", train_labels, \"\\n\")\n",
    "print(\"test labels:\", test_labels, \"\\n\")\n",
    "\n",
    "train_wav = []\n",
    "test_wav = []\n",
    "\n",
    "for label in train_labels:\n",
    "    train_wav.append([wav for wav in train.joinpath(label).iterdir() if wav.name != \".DS_Store\"])\n",
    "    \n",
    "for label in test_labels:\n",
    "    test_wav.append([wav for wav in test.joinpath(label).iterdir() if wav.name != \".DS_Store\"])\n",
    "    \n",
    "print(len(train_wav), len(train_wav[0]))\n",
    "print(len(test_wav), len(test_wav[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Seq2Seq Dataset. Uses torchaudio + mu law to process wav files.\n",
    "    Takes first 160,000 samples (~4s), and samples every 5 to get processed audio tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, wavs, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels: list of labels\n",
    "            wavs: list of paths to our wav files\n",
    "        \"\"\"\n",
    "        self.labels = labels\n",
    "        self.wavs = wavs\n",
    "        self.dict = {'Beethoven_Accompanied_Violin':0, 'Bach_Solo_Piano':1, 'Bach_Solo_Cello':2, 'Beethoven_Solo_Piano':3, 'Beethoven_String_Quartet':4, 'Cambini_Wind_Quintet':5}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, rate = librosa.load(self.wavs[index], sr=16000, duration=10)\n",
    "        assert rate == 16000\n",
    "        sample_tensor = torch.tensor(data).float()\n",
    "        assert sample_tensor.size()  == torch.Size([160000])\n",
    "        downsampled_tensor = sample_tensor[::5]\n",
    "        \n",
    "        return downsampled_tensor, torch.tensor(self.dict[self.labels[index]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
