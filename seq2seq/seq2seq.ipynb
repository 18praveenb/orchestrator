{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting between instruments using a Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import librosa\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torchaudio processing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized(tensor):\n",
    "    # https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html\n",
    "    centered = tensor - tensor.mean()\n",
    "    normalized = tensor / tensor.abs().max()\n",
    "    return normalized\n",
    "\n",
    "_mu_encoder = torchaudio.transforms.MuLawEncoding()\n",
    "_mu_decoder = torchaudio.transforms.MuLawDecoding()\n",
    "\n",
    "def mu_law_encode(waveform):\n",
    "    return _mu_encoder(normalized(waveform))\n",
    "\n",
    "def mu_law_decode(waveform):\n",
    "    return _mu_decoder(normalized(waveform))\n",
    "\n",
    "def load_audio(path):\n",
    "    \"\"\" Load .wav file to Mu law encoding tensor \"\"\"\n",
    "    waveform, sample_rate = torchaudio.load(path)\n",
    "    return mu_law_encode(waveform), sample_rate\n",
    "\n",
    "def save_audio(path, data, sample_rate):\n",
    "    \"\"\" Save Mu law encoding tensor to .wav file \"\"\"\n",
    "    waveform = mu_law_decode(data)\n",
    "    torchaudio.save(path, data, sample_rate)\n",
    "    \n",
    "SOS_token = -1\n",
    "EOS_token = -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting wav files to tensors and saving them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Path.cwd().joinpath(\"dataset\")\n",
    "train_wav = dataset.joinpath(\"train_wav\")\n",
    "train_tensors = dataset.joinpath(\"train_tensors\")\n",
    "test_wav = dataset.joinpath(\"test_wav\")\n",
    "test_tensors = dataset.joinpath(\"test_tensors\")\n",
    "\n",
    "for instrument in train_wav.iterdir():\n",
    "    if (instrument.name != \".DS_Store\"):\n",
    "        for note in train_wav.joinpath(instrument.name).iterdir():\n",
    "            if (note.name != \".DS_Store\"):\n",
    "                wav_tensor = load_audio(train_wav.joinpath(instrument.name, note.name))\n",
    "                wav_tensor = (wav_tensor[0][0] + wav_tensor[0][1]) / 2\n",
    "                torch.save(wav_tensor, train_tensors.joinpath(instrument.name, note.stem + \".pt\"))\n",
    "                \n",
    "for instrument in test_wav.iterdir():\n",
    "    if (instrument.name != \".DS_Store\"):\n",
    "        for note in test_wav.joinpath(instrument.name).iterdir():\n",
    "            if (note.name != \".DS_Store\"):\n",
    "                wav_tensor = load_audio(test_wav.joinpath(instrument.name, note.name))\n",
    "                wav_tensor = (wav_tensor[0][0] + wav_tensor[0][1]) / 2\n",
    "                torch.save(wav_tensor, test_tensors.joinpath(instrument.name, note.stem + \".pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in the tensors and preparing them for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train lengths: 45 45\n",
      "test lengths: 15 15\n"
     ]
    }
   ],
   "source": [
    "dataset = Path.cwd().joinpath(\"dataset\")\n",
    "\n",
    "#Change for model you are training\n",
    "train_input = dataset.joinpath(\"train_tensors\", \"75\")\n",
    "train_output = dataset.joinpath(\"train_tensors\", \"81\")\n",
    "test_input = dataset.joinpath(\"test_tensors\", \"75\")\n",
    "test_output = dataset.joinpath(\"test_tensors\", \"81\")\n",
    "\n",
    "train_input_wav = []\n",
    "train_output_wav = []\n",
    "test_input_wav = []\n",
    "test_output_wav = []\n",
    "\n",
    "#Take out DS_Stores if you are using a Mac\n",
    "for tensor in train_input.iterdir():\n",
    "    if tensor.name != \".DS_Store\":\n",
    "        train_input_wav.append(tensor)\n",
    "    \n",
    "for tensor in train_output.iterdir():\n",
    "    if tensor.name != \".DS_Store\":\n",
    "        train_output_wav.append(tensor)\n",
    "\n",
    "for tensor in test_input.iterdir():\n",
    "    if tensor.name != \".DS_Store\":\n",
    "        test_input_wav.append(tensor)\n",
    "    \n",
    "for tensor in test_output.iterdir():\n",
    "    if tensor.name != \".DS_Store\":\n",
    "        test_output_wav.append(tensor)\n",
    "        \n",
    "print(\"train lengths:\", len(train_input_wav), len(train_output_wav))\n",
    "print(\"test lengths:\", len(test_input_wav), len(test_output_wav))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Seq2Seq Dataset. Uses torchaudio + mu law to process wav files.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_wavs, output_wavs, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_wavs: list of input wavs from 1st instrument\n",
    "            output_wavs: list of output wavs from 2nd instrument\n",
    "        \"\"\"\n",
    "        self.input_wavs = input_wavs\n",
    "        self.output_wavs = output_wavs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_wavs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = torch.load(self.input_wavs[index])\n",
    "        output = torch.load(self.output_wavs[index])\n",
    "        return torch.cat((input[::100], torch.tensor([EOS_token])), 0), torch.cat((output[::100], torch.tensor([EOS_token])), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data loaders using Seq2SeqDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 45\n",
      "Test set size: 15\n"
     ]
    }
   ],
   "source": [
    "train_set = Seq2SeqDataset(train_input_wav, train_output_wav)\n",
    "test_set = Seq2SeqDataset(test_input_wav, test_output_wav)\n",
    "print(\"Train set size: \" + str(len(train_set)))\n",
    "print(\"Test set size: \" + str(len(test_set)))\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {} #needed for using datasets on gpu\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 8, shuffle = True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 8, shuffle = True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([224])\n",
      "torch.Size([224])\n",
      "torch.Size([224])\n",
      "torch.Size([224])\n",
      "torch.Size([224])\n",
      "torch.Size([224])\n"
     ]
    }
   ],
   "source": [
    "for data, target in train_loader:\n",
    "    print(data[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, seq_length):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.gru = nn.GRU(1, 1, num_layers = 1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(seq_length, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def trainx(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=200):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        print(input_tensor[ei])\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, train_loader, n_iters, print_every=5, plot_every=2, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        for input, target in train_loader:\n",
    "            input = torch.flatten(input).view(-1, 1)\n",
    "            target = torch.flatten(target).view(-1, 1)\n",
    "            loss = train(input, target, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            if iter % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                             iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "            if iter % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=0.01)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=0.01)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, epoch):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "        data = torch.unsqueeze(data, 2)\n",
    "        target = torch.unsqueeze(target, 2)\n",
    "        data = data.permute(1, 0, 2)\n",
    "        target = target.permute(1, 0, 2)\n",
    "        \n",
    "        print(data)\n",
    "        data = data.permute(1, 0, 2)\n",
    "        data = data.requires_grad_() #set requires_grad to True for training\n",
    "        output = model(data)\n",
    "        output = output.permute(1, 0, 2) #original output dimensions are batchSizex1x6 \n",
    "#         print(output[0].shape, target.shape)\n",
    "        loss = F.nll_loss(output[0], target) #the loss functions expects a batchSizex10 input\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0: #print training stats\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[122],\n",
      "         [122],\n",
      "         [121],\n",
      "         ...,\n",
      "         [122],\n",
      "         [121],\n",
      "         [122]],\n",
      "\n",
      "        [[128],\n",
      "         [128],\n",
      "         [124],\n",
      "         ...,\n",
      "         [128],\n",
      "         [128],\n",
      "         [128]],\n",
      "\n",
      "        [[128],\n",
      "         [122],\n",
      "         [128],\n",
      "         ...,\n",
      "         [128],\n",
      "         [128],\n",
      "         [122]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 12],\n",
      "         [ 23],\n",
      "         [ 18],\n",
      "         ...,\n",
      "         [233],\n",
      "         [  9],\n",
      "         [ 16]],\n",
      "\n",
      "        [[ 12],\n",
      "         [ 66],\n",
      "         [225],\n",
      "         ...,\n",
      "         [ 25],\n",
      "         [ 18],\n",
      "         [219]],\n",
      "\n",
      "        [[ 14],\n",
      "         [236],\n",
      "         [237],\n",
      "         ...,\n",
      "         [ 12],\n",
      "         [231],\n",
      "         [204]]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "only Tensors of floating point dtype can require gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-10669bd96154>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m223\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m223\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-115-2f1d15e55ebf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, epoch)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#set requires_grad to True for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#original output dimensions are batchSizex1x6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: only Tensors of floating point dtype can require gradients"
     ]
    }
   ],
   "source": [
    "hidden_size = 200\n",
    "encoder = EncoderRNN(223).to(device)\n",
    "decoder = EncoderRNN(223).to(device)\n",
    "train(encoder, decoder, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
