{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a Pytorch Dataset for the Music Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import librosa\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train_wav and test_wav lists of lists that hold the path to every wav file in MusicNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels: ['Bach_Solo_Cello', 'Bach_Solo_Piano', 'Beethoven_Accompanied_Violin', 'Beethoven_Solo_Piano', 'Beethoven_String_Quartet', 'Cambini_Wind_Quintet'] \n",
      "\n",
      "test labels: ['Bach_Solo_Cello', 'Bach_Solo_Piano', 'Beethoven_Accompanied_Violin', 'Beethoven_Solo_Piano', 'Beethoven_String_Quartet', 'Cambini_Wind_Quintet'] \n",
      "\n",
      "6 9\n",
      "6 2\n"
     ]
    }
   ],
   "source": [
    "dataset = Path.cwd().parent.joinpath(\"musicnet\", \"music_classification_data\")\n",
    "\n",
    "train = dataset.joinpath(\"train\")\n",
    "test = dataset.joinpath(\"test\")\n",
    "\n",
    "train_labels = [p.stem for p in train.iterdir()]\n",
    "test_labels = [p.stem for p in test.iterdir()]\n",
    "\n",
    "# Uncomment the code below if needed for your machine\n",
    "# train_labels.remove(\".DS_Store\")\n",
    "# test_labels.remove(\".DS_Store\")\n",
    "\n",
    "print(\"train labels:\", train_labels, \"\\n\")\n",
    "print(\"test labels:\", test_labels, \"\\n\")\n",
    "\n",
    "train_wav = []\n",
    "test_wav = []\n",
    "\n",
    "for label in train_labels:\n",
    "    train_wav.append([wav for wav in train.joinpath(label).iterdir() if wav.name != \".DS_Store\"])\n",
    "    \n",
    "for label in test_labels:\n",
    "    test_wav.append([wav for wav in test.joinpath(label).iterdir() if wav.name != \".DS_Store\"])\n",
    "    \n",
    "print(len(train_wav), len(train_wav[0]))\n",
    "print(len(test_wav), len(test_wav[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puts our test_wav and train_wav into 1d arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 160\n",
      "25 25\n"
     ]
    }
   ],
   "source": [
    "train_y = []\n",
    "test_y = []\n",
    "\n",
    "unprocessed_train_x = []\n",
    "unprocessed_test_x = []\n",
    "\n",
    "for i in range(len(train_labels)):\n",
    "    for j in range(len(train_wav[i])):\n",
    "        train_y.append(train_labels[i])\n",
    "        \n",
    "for i in range(len(test_labels)):\n",
    "    for j in range(len(test_wav[i])):\n",
    "        test_y.append(test_labels[i])\n",
    "        \n",
    "for arr in train_wav:\n",
    "    unprocessed_train_x.extend(arr)\n",
    "    \n",
    "for arr in test_wav:\n",
    "    unprocessed_test_x.extend(arr)\n",
    "        \n",
    "print(len(train_y), len(unprocessed_train_x))\n",
    "print(len(test_y), len(unprocessed_test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicClassifierDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Music Classifier Dataset. Uses librosa to process wav files.\n",
    "    Takes first 160,000 samples (~4s), and samples every 5 to get processed audio tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, wavs, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels: list of labels\n",
    "            wavs: list of paths to our wav files\n",
    "        \"\"\"\n",
    "        self.labels = labels\n",
    "        self.wavs = wavs\n",
    "        self.dict = {'Beethoven_Accompanied_Violin':0, 'Bach_Solo_Piano':1, 'Bach_Solo_Cello':2, 'Beethoven_Solo_Piano':3, 'Beethoven_String_Quartet':4, 'Cambini_Wind_Quintet':5}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, rate = librosa.load(self.wavs[index], sr=16000, duration=10)\n",
    "        assert rate == 16000\n",
    "        sample_tensor = torch.tensor(data).float()\n",
    "        assert sample_tensor.size()  == torch.Size([160000])\n",
    "        downsampled_tensor = sample_tensor[::5]\n",
    "        \n",
    "        return downsampled_tensor, torch.tensor(self.dict[self.labels[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 160\n",
      "Test set size: 25\n"
     ]
    }
   ],
   "source": [
    "train_set = MusicClassifierDataset(unprocessed_train_x, train_y)\n",
    "test_set = MusicClassifierDataset(unprocessed_test_x, test_y)\n",
    "print(\"Train set size: \" + str(len(train_set)))\n",
    "print(\"Test set size: \" + str(len(test_set)))\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {} #needed for using datasets on gpu\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 128, shuffle = True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 128, shuffle = True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv1d(1, 128, kernel_size=(80,), stride=(4,))\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(128, 256, kernel_size=(3,), stride=(1,))\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(256, 512, kernel_size=(3,), stride=(1,))\n",
      "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (avgPool): AvgPool1d(kernel_size=(30,), stride=(30,), padding=(0,))\n",
      "  (fc1): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 128, 80, 4)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(128, 128, 3)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(128, 256, 3)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(256, 512, 3)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.avgPool = nn.AvgPool1d(30) #input should be 512x30 so this outputs a 512x1\n",
    "        self.fc1 = nn.Linear(512, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = self.avgPool(x)\n",
    "        x = x.permute(0, 2, 1) #change the 512x1 to 1x512\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim = 2)\n",
    "\n",
    "model = Net()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data = torch.unsqueeze(data, 0)\n",
    "        data = data.permute(1, 0, 2)\n",
    "        data = data.requires_grad_() #set requires_grad to True for training\n",
    "        output = model(data)\n",
    "        output = output.permute(1, 0, 2) #original output dimensions are batchSizex1x10 \n",
    "#         print(output[0].shape, target.shape)\n",
    "        loss = F.nll_loss(output[0], target) #the loss functions expects a batchSizex10 input\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0: #print training stats\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data = torch.unsqueeze(data, 0)\n",
    "        data = data.permute(1, 0, 2)\n",
    "        output = model(data)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        pred = output.max(2)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target).cpu().sum().item()\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/160 (0%)]\tLoss: 2.251221\n",
      "\n",
      "Test set: Accuracy: 3/25 (12%)\n",
      "\n",
      "Train Epoch: 2 [0/160 (0%)]\tLoss: 1.646285\n",
      "\n",
      "Test set: Accuracy: 10/25 (40%)\n",
      "\n",
      "Train Epoch: 3 [0/160 (0%)]\tLoss: 1.625549\n",
      "\n",
      "Test set: Accuracy: 10/25 (40%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_interval = 20\n",
    "for epoch in range(1, 41):\n",
    "    if epoch == 31:\n",
    "        print(\"First round of training complete. Setting learn rate to 0.001.\")\n",
    "    scheduler.step()\n",
    "    train(model, epoch)\n",
    "    test(model, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
