{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the data for this classifier:\n",
    "\n",
    "1. Download the Musicnet library\n",
    "2. Put the folder titled \"musicnet\" in the music-translation folder\n",
    "3. Create a folder titled \"music_classification_data\" in the \"musicnet\" folder\n",
    "4. Create two folders in \"music_classification_data\" titled \"test\" and \"train\"\n",
    "5. In the \"test\" and \"train\" folders, put folders with the wav files in each such that the folders are titled with the labels for the wav files (e.g. \"Beethoven_Accompanied_Violin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check current device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puts data and labels into 2d arrays, splits into train and test:\n",
    "\n",
    "Tutorial on pathlib libary: https://realpython.com/python-pathlib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\aliwa\\\\Desktop\\\\orchestrator\\\\music-translation\\\\musicnet\\\\music_classification_data\\\\train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-8c9ebfe77c53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-8c9ebfe77c53>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\pathlib.py\u001b[0m in \u001b[0;36miterdir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1081\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1083\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'..'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m                 \u001b[1;31m# Yielding a path object for these makes little sense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\aliwa\\\\Desktop\\\\orchestrator\\\\music-translation\\\\musicnet\\\\music_classification_data\\\\train'"
     ]
    }
   ],
   "source": [
    "dataset = Path.cwd().parent.joinpath(\"musicnet\", \"music_classification_data\")\n",
    "\n",
    "train = dataset.joinpath(\"train\")\n",
    "test = dataset.joinpath(\"test\")\n",
    "\n",
    "train_labels = [p.stem for p in train.iterdir()]\n",
    "test_labels = [p.stem for p in test.iterdir()]\n",
    "\n",
    "train_labels.remove(\".DS_Store\")\n",
    "test_labels.remove(\".DS_Store\")\n",
    "\n",
    "print(\"train labels:\", train_labels, \"\\n\")\n",
    "print(\"test labels:\", test_labels, \"\\n\")\n",
    "\n",
    "train_wav = []\n",
    "test_wav = []\n",
    "\n",
    "for label in train_labels:\n",
    "    train_wav.append([wav for wav in train.joinpath(label).iterdir() if wav.name != \".DS_Store\"])\n",
    "    \n",
    "for label in test_labels:\n",
    "    test_wav.append([wav for wav in test.joinpath(label).iterdir() if wav.name != \".DS_Store\"])\n",
    "    \n",
    "print(len(train_wav), len(train_wav[0]))\n",
    "print(len(test_wav), len(test_wav[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puts data and labels into 1d arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-6a1ec172b654>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0munprocessed_test_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_wav\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_labels' is not defined"
     ]
    }
   ],
   "source": [
    "train_y = []\n",
    "test_y = []\n",
    "\n",
    "unprocessed_train_x = []\n",
    "unprocessed_test_x = []\n",
    "\n",
    "for i in range(len(train_labels)):\n",
    "    for j in range(len(train_wav[i])):\n",
    "        train_y.append(train_labels[i])\n",
    "        \n",
    "for i in range(len(test_labels)):\n",
    "    for j in range(len(test_wav[i])):\n",
    "        test_y.append(test_labels[i])\n",
    "        \n",
    "for arr in train_wav:\n",
    "    unprocessed_train_x.extend(arr)\n",
    "    \n",
    "for arr in test_wav:\n",
    "    unprocessed_test_x.extend(arr)\n",
    "        \n",
    "print(len(train_y), len(unprocessed_train_x))\n",
    "print(len(test_y), len(unprocessed_test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processed data, uses librosa to turn wav file into a tensor. Takes first 160,000 samples (~4s), and samples every 5 to get processed audio tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train wav files:\n",
      "torch.Size([8640471])\n",
      "1 / 153 processed\n",
      "torch.Size([8112170])\n",
      "2 / 153 processed\n",
      "torch.Size([14573819])\n",
      "3 / 153 processed\n",
      "torch.Size([4891795])\n",
      "4 / 153 processed\n",
      "torch.Size([5874835])\n",
      "5 / 153 processed\n",
      "torch.Size([4955743])\n",
      "6 / 153 processed\n",
      "torch.Size([7830884])\n",
      "7 / 153 processed\n",
      "torch.Size([4283664])\n",
      "8 / 153 processed\n",
      "torch.Size([6760072])\n",
      "9 / 153 processed\n",
      "torch.Size([6552347])\n",
      "10 / 153 processed\n",
      "torch.Size([3165205])\n",
      "11 / 153 processed\n",
      "torch.Size([8514665])\n",
      "12 / 153 processed\n",
      "torch.Size([3076598])\n",
      "13 / 153 processed\n",
      "torch.Size([7374054])\n",
      "14 / 153 processed\n",
      "torch.Size([9968745])\n",
      "15 / 153 processed\n",
      "torch.Size([3787129])\n",
      "16 / 153 processed\n",
      "torch.Size([1816451])\n",
      "17 / 153 processed\n",
      "torch.Size([1717395])\n",
      "18 / 153 processed\n",
      "torch.Size([1569437])\n",
      "19 / 153 processed\n",
      "torch.Size([3913352])\n",
      "20 / 153 processed\n",
      "torch.Size([1862845])\n",
      "21 / 153 processed\n",
      "torch.Size([2148311])\n",
      "22 / 153 processed\n",
      "torch.Size([1920523])\n",
      "23 / 153 processed\n",
      "torch.Size([3795488])\n",
      "24 / 153 processed\n",
      "torch.Size([3166877])\n",
      "25 / 153 processed\n",
      "torch.Size([2853408])\n",
      "26 / 153 processed\n",
      "torch.Size([960053])\n",
      "27 / 153 processed\n",
      "torch.Size([2768562])\n",
      "28 / 153 processed\n",
      "torch.Size([1434018])\n",
      "29 / 153 processed\n",
      "torch.Size([1855739])\n",
      "30 / 153 processed\n",
      "torch.Size([1674345])\n",
      "31 / 153 processed\n",
      "torch.Size([1481248])\n",
      "32 / 153 processed\n",
      "torch.Size([1299854])\n",
      "33 / 153 processed\n",
      "torch.Size([1207485])\n",
      "34 / 153 processed\n",
      "torch.Size([1747070])\n",
      "35 / 153 processed\n",
      "torch.Size([2572539])\n",
      "36 / 153 processed\n",
      "torch.Size([1601620])\n",
      "37 / 153 processed\n",
      "torch.Size([2481842])\n",
      "38 / 153 processed\n",
      "torch.Size([1932644])\n",
      "39 / 153 processed\n",
      "torch.Size([1024836])\n",
      "40 / 153 processed\n",
      "torch.Size([1740800])\n",
      "41 / 153 processed\n",
      "torch.Size([1353352])\n",
      "42 / 153 processed\n",
      "torch.Size([1850306])\n",
      "43 / 153 processed\n",
      "torch.Size([3418907])\n",
      "44 / 153 processed\n",
      "torch.Size([2070988])\n",
      "45 / 153 processed\n",
      "torch.Size([1257222])\n",
      "46 / 153 processed\n",
      "torch.Size([2980467])\n",
      "47 / 153 processed\n",
      "torch.Size([3655054])\n",
      "48 / 153 processed\n",
      "torch.Size([4962430])\n",
      "49 / 153 processed\n",
      "torch.Size([3232497])\n",
      "50 / 153 processed\n",
      "torch.Size([3858600])\n",
      "51 / 153 processed\n",
      "torch.Size([4832027])\n",
      "52 / 153 processed\n",
      "torch.Size([3199478])\n",
      "53 / 153 processed\n",
      "torch.Size([4175831])\n",
      "54 / 153 processed\n",
      "torch.Size([2207661])\n",
      "55 / 153 processed\n",
      "torch.Size([2065137])\n",
      "56 / 153 processed\n",
      "torch.Size([7578854])\n",
      "57 / 153 processed\n",
      "torch.Size([7194750])\n",
      "58 / 153 processed\n",
      "torch.Size([5015511])\n",
      "59 / 153 processed\n",
      "torch.Size([6385999])\n",
      "60 / 153 processed\n",
      "torch.Size([8862825])\n",
      "61 / 153 processed\n",
      "torch.Size([6484219])\n",
      "62 / 153 processed\n",
      "torch.Size([3464464])\n",
      "63 / 153 processed\n",
      "torch.Size([9792784])\n",
      "64 / 153 processed\n",
      "torch.Size([7899429])\n",
      "65 / 153 processed\n",
      "torch.Size([6194574])\n",
      "66 / 153 processed\n",
      "torch.Size([8365872])\n",
      "67 / 153 processed\n",
      "torch.Size([4876330])\n",
      "68 / 153 processed\n",
      "torch.Size([3284324])\n",
      "69 / 153 processed\n",
      "torch.Size([4751778])\n",
      "70 / 153 processed\n",
      "torch.Size([2206825])\n",
      "71 / 153 processed\n",
      "torch.Size([4475507])\n",
      "72 / 153 processed\n",
      "torch.Size([3657143])\n",
      "73 / 153 processed\n",
      "torch.Size([5097849])\n",
      "74 / 153 processed\n",
      "torch.Size([4862120])\n",
      "75 / 153 processed\n",
      "torch.Size([6776373])\n",
      "76 / 153 processed\n",
      "torch.Size([3446492])\n",
      "77 / 153 processed\n",
      "torch.Size([5364507])\n",
      "78 / 153 processed\n",
      "torch.Size([7256608])\n",
      "79 / 153 processed\n",
      "torch.Size([3394665])\n",
      "80 / 153 processed\n",
      "torch.Size([7233620])\n",
      "81 / 153 processed\n",
      "torch.Size([7419612])\n",
      "82 / 153 processed\n",
      "torch.Size([6495086])\n",
      "83 / 153 processed\n",
      "torch.Size([4408216])\n",
      "84 / 153 processed\n",
      "torch.Size([3533845])\n",
      "85 / 153 processed\n",
      "torch.Size([7060585])\n",
      "86 / 153 processed\n",
      "torch.Size([7590975])\n",
      "87 / 153 processed\n",
      "torch.Size([6206694])\n",
      "88 / 153 processed\n",
      "torch.Size([17087426])\n",
      "89 / 153 processed\n",
      "torch.Size([6177855])\n",
      "90 / 153 processed\n",
      "torch.Size([8306939])\n",
      "91 / 153 processed\n",
      "torch.Size([5297215])\n",
      "92 / 153 processed\n",
      "torch.Size([8774636])\n",
      "93 / 153 processed\n",
      "torch.Size([8992810])\n",
      "94 / 153 processed\n",
      "torch.Size([10335713])\n",
      "95 / 153 processed\n",
      "torch.Size([12330632])\n",
      "96 / 153 processed\n",
      "torch.Size([5107880])\n",
      "97 / 153 processed\n",
      "torch.Size([5435142])\n",
      "98 / 153 processed\n",
      "torch.Size([7170508])\n",
      "99 / 153 processed\n",
      "torch.Size([7691285])\n",
      "100 / 153 processed\n",
      "torch.Size([15873672])\n",
      "101 / 153 processed\n",
      "torch.Size([7749800])\n",
      "102 / 153 processed\n",
      "torch.Size([12718498])\n",
      "103 / 153 processed\n",
      "torch.Size([2800745])\n",
      "104 / 153 processed\n",
      "torch.Size([8821447])\n",
      "105 / 153 processed\n",
      "torch.Size([16562051])\n",
      "106 / 153 processed\n",
      "torch.Size([4590864])\n",
      "107 / 153 processed\n",
      "torch.Size([2884755])\n",
      "108 / 153 processed\n",
      "torch.Size([3244200])\n",
      "109 / 153 processed\n",
      "torch.Size([5722698])\n",
      "110 / 153 processed\n",
      "torch.Size([5326054])\n",
      "111 / 153 processed\n",
      "torch.Size([10415961])\n",
      "112 / 153 processed\n",
      "torch.Size([2423328])\n",
      "113 / 153 processed\n",
      "torch.Size([5531690])\n",
      "114 / 153 processed\n",
      "torch.Size([10950113])\n",
      "115 / 153 processed\n",
      "torch.Size([4419919])\n",
      "116 / 153 processed\n",
      "torch.Size([3864869])\n",
      "117 / 153 processed\n",
      "torch.Size([6130626])\n",
      "118 / 153 processed\n",
      "torch.Size([7001235])\n",
      "119 / 153 processed\n",
      "torch.Size([7088170])\n",
      "120 / 153 processed\n",
      "torch.Size([4763481])\n",
      "121 / 153 processed\n",
      "torch.Size([12196049])\n",
      "122 / 153 processed\n",
      "torch.Size([3579403])\n",
      "123 / 153 processed\n",
      "torch.Size([6375550])\n",
      "124 / 153 processed\n",
      "torch.Size([6273568])\n",
      "125 / 153 processed\n",
      "torch.Size([7574257])\n",
      "126 / 153 processed\n",
      "torch.Size([5374538])\n",
      "127 / 153 processed\n",
      "torch.Size([4216373])\n",
      "128 / 153 processed\n",
      "torch.Size([7649489])\n",
      "129 / 153 processed\n",
      "torch.Size([12285493])\n",
      "130 / 153 processed\n",
      "torch.Size([9300846])\n",
      "131 / 153 processed\n",
      "torch.Size([6437826])\n",
      "132 / 153 processed\n",
      "torch.Size([5730639])\n",
      "133 / 153 processed\n",
      "torch.Size([15058652])\n",
      "134 / 153 processed\n",
      "torch.Size([9089777])\n",
      "135 / 153 processed\n",
      "torch.Size([17104562])\n",
      "136 / 153 processed\n",
      "torch.Size([9501885])\n",
      "137 / 153 processed\n",
      "torch.Size([6749565])\n",
      "138 / 153 processed\n",
      "torch.Size([8968987])\n",
      "139 / 153 processed\n",
      "torch.Size([5124598])\n",
      "140 / 153 processed\n",
      "torch.Size([7383667])\n",
      "141 / 153 processed\n",
      "torch.Size([1887922])\n",
      "142 / 153 processed\n",
      "torch.Size([6141075])\n",
      "143 / 153 processed\n",
      "torch.Size([6954423])\n",
      "144 / 153 processed\n",
      "torch.Size([3102094])\n",
      "145 / 153 processed\n",
      "torch.Size([5348206])\n",
      "146 / 153 processed\n",
      "torch.Size([3405950])\n",
      "147 / 153 processed\n",
      "torch.Size([3960164])\n",
      "148 / 153 processed\n",
      "torch.Size([4001960])\n",
      "149 / 153 processed\n",
      "torch.Size([5264196])\n",
      "150 / 153 processed\n",
      "torch.Size([3742407])\n",
      "151 / 153 processed\n",
      "torch.Size([4459207])\n",
      "152 / 153 processed\n",
      "torch.Size([5695948])\n",
      "153 / 153 processed\n",
      "Processing test wav files:\n",
      "torch.Size([3216614])\n",
      "1 / 50 processed\n",
      "torch.Size([5441829])\n",
      "2 / 50 processed\n",
      "torch.Size([8287295])\n",
      "3 / 50 processed\n",
      "torch.Size([1994920])\n",
      "4 / 50 processed\n",
      "torch.Size([9503138])\n",
      "5 / 50 processed\n",
      "torch.Size([5168484])\n",
      "6 / 50 processed\n",
      "torch.Size([3810952])\n",
      "7 / 50 processed\n",
      "torch.Size([1482502])\n",
      "8 / 50 processed\n",
      "torch.Size([1733277])\n",
      "9 / 50 processed\n",
      "torch.Size([2195122])\n",
      "10 / 50 processed\n",
      "torch.Size([1042391])\n",
      "11 / 50 processed\n",
      "torch.Size([883984])\n",
      "12 / 50 processed\n",
      "torch.Size([3141800])\n",
      "13 / 50 processed\n",
      "torch.Size([1949362])\n",
      "14 / 50 processed\n",
      "torch.Size([1461186])\n",
      "15 / 50 processed\n",
      "torch.Size([2456347])\n",
      "16 / 50 processed\n",
      "torch.Size([4670694])\n",
      "17 / 50 processed\n",
      "torch.Size([5348206])\n",
      "18 / 50 processed\n",
      "torch.Size([4151171])\n",
      "19 / 50 processed\n",
      "torch.Size([10783765])\n",
      "20 / 50 processed\n",
      "torch.Size([9012036])\n",
      "21 / 50 processed\n",
      "torch.Size([10162678])\n",
      "22 / 50 processed\n",
      "torch.Size([6691945])\n",
      "23 / 50 processed\n",
      "torch.Size([7124115])\n",
      "24 / 50 processed\n",
      "torch.Size([7557956])\n",
      "25 / 50 processed\n",
      "torch.Size([8206211])\n",
      "26 / 50 processed\n",
      "torch.Size([5470250])\n",
      "27 / 50 processed\n",
      "torch.Size([3632066])\n",
      "28 / 50 processed\n",
      "torch.Size([6251834])\n",
      "29 / 50 processed\n",
      "torch.Size([3815968])\n",
      "30 / 50 processed\n",
      "torch.Size([5826769])\n",
      "31 / 50 processed\n",
      "torch.Size([2871798])\n",
      "32 / 50 processed\n",
      "torch.Size([4923142])\n",
      "33 / 50 processed\n",
      "torch.Size([6248490])\n",
      "34 / 50 processed\n",
      "torch.Size([2557911])\n",
      "35 / 50 processed\n",
      "torch.Size([5109134])\n",
      "36 / 50 processed\n",
      "torch.Size([5212787])\n",
      "37 / 50 processed\n",
      "torch.Size([5289692])\n",
      "38 / 50 processed\n",
      "torch.Size([6071276])\n",
      "39 / 50 processed\n",
      "torch.Size([9088523])\n",
      "40 / 50 processed\n",
      "torch.Size([5879014])\n",
      "41 / 50 processed\n",
      "torch.Size([4707057])\n",
      "42 / 50 processed\n",
      "torch.Size([5671289])\n",
      "43 / 50 processed\n",
      "torch.Size([4753032])\n",
      "44 / 50 processed\n",
      "torch.Size([6329992])\n",
      "45 / 50 processed\n",
      "torch.Size([10378763])\n",
      "46 / 50 processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8115932])\n",
      "47 / 50 processed\n",
      "torch.Size([4798590])\n",
      "48 / 50 processed\n",
      "torch.Size([3920040])\n",
      "49 / 50 processed\n",
      "torch.Size([5468996])\n",
      "50 / 50 processed\n"
     ]
    }
   ],
   "source": [
    "train_x = []\n",
    "test_x = []\n",
    "\n",
    "train_progress_counter = 0\n",
    "test_progress_counter = 0\n",
    "\n",
    "print(\"Processing train wav files:\")\n",
    "\n",
    "for path in unprocessed_train_x:\n",
    "    data, rate = librosa.load(path, sr=16000)\n",
    "    assert rate == 16000\n",
    "    sample_tensor = torch.tensor(data).float()\n",
    "    short_tensor = sample_tensor[:160000]\n",
    "    downsampled_tensor = short_tensor[::5]\n",
    "    print(sample_tensor.size())\n",
    "    train_progress_counter += 1\n",
    "    if train_progress_counter % 10 == 0:\n",
    "        print(train_progress_counter, \"/\", len(unprocessed_train_x), \"processed\")\n",
    "    train_x.append(downsampled_tensor)\n",
    "    \n",
    "print(\"\\n\", \"Processing test wav files:\")\n",
    "    \n",
    "for path in unprocessed_test_x:\n",
    "    data, rate = librosa.load(path, sr=16000)\n",
    "    assert rate == 16000\n",
    "    sample_tensor = torch.tensor(data).float()\n",
    "    short_tensor = sample_tensor[:160000]\n",
    "    downsampled_tensor = short_tensor[::5]\n",
    "    print(sample_tensor.size())\n",
    "    test_progress_counter += 1\n",
    "    if test_progress_counter % 10 == 0:\n",
    "        print(test_progress_counter, \"/\", len(unprocessed_test_x), \"processed\")\n",
    "    test_x.append(downsampled_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save train_x and test_x to .pt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_x, \"train_x.pt\")\n",
    "torch.save(test_x, \"test_x.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retreive train_x and test_x from saved .pt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32000])\n",
      "torch.Size([32000])\n"
     ]
    }
   ],
   "source": [
    "train_x = torch.load(\"train_x.pt\")\n",
    "test_x = torch.load(\"test_x.pt\")\n",
    "\n",
    "print(train_x[0])\n",
    "print(test_x[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make train_x so that it has labels + data, same w test_x\n",
    "\n",
    "\n",
    "# Skipping the batching for now\n",
    "\n",
    "# kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {} #needed for using datasets on gpu\n",
    "# train_loader = torch.utils.data.DataLoader(train_x, batch_size = 128, shuffle = True, **kwargs)\n",
    "# test_loader = torch.utils.data.DataLoader(test_x, batch_size = 128, shuffle = True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN modeled after the M5 network architecture described in https://arxiv.org/pdf/1610.00087.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv1d(1, 128, kernel_size=(80,), stride=(4,))\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(128, 256, kernel_size=(3,), stride=(1,))\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(256, 512, kernel_size=(3,), stride=(1,))\n",
      "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (avgPool): AvgPool1d(kernel_size=(30,), stride=(30,), padding=(0,))\n",
      "  (fc1): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 128, 80, 4)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(128, 128, 3)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(128, 256, 3)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(256, 512, 3)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.avgPool = nn.AvgPool1d(30) #input should be 512x30 so this outputs a 512x1\n",
    "        self.fc1 = nn.Linear(512, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = self.avgPool(x)\n",
    "        x = x.permute(0, 2, 1) #change the 512x1 to 1x512\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim = 2)\n",
    "\n",
    "model = CNN()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optimizer (using Adam) and scheduler, which lowers the learning rate from 0.01 to 0.0001 over the course of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    for data, target in train_x:\n",
    "        # for batch_idx\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        data = data.requires_grad_() #set requires_grad to True for training\n",
    "        output = model(data)\n",
    "        output = output.permute(1, 0, 2) #original output dimensions are batchSizex1x10 \n",
    "        loss = F.nll_loss(output[0], target) #the loss functions expects a batchSizex10 input\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         if batch_idx % log_interval == 0: #print training stats\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        pred = output.max(2)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target).cpu().sum().item()\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-9cbe6e7efc52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"First round of training complete. Setting learn rate to 0.001.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-d4a5bf54ad2d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, epoch)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[1;31m# for batch_idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "log_interval = 20\n",
    "for epoch in range(1, 41):\n",
    "    if epoch == 31:\n",
    "        print(\"First round of training complete. Setting learn rate to 0.001.\")\n",
    "    scheduler.step()\n",
    "    train(model, epoch)\n",
    "    test(model, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
