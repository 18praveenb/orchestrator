# Week 1 Meeting 2/17

- No meeting next week (due to retreat)
- Regular schedule TBD (when2meet!)

## Pytorch (4:00 - 5:30)
- [Pytorch 60min blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)
- Check the advice sections below for what to focus on in the tutorial.
- Would recommend either running the Jupyter notebook or using Google Colab and setting the runtime to 'Hardware accelerator: GPU' if you want GPU access
- Should have Colab and Jupyter links on each page of the tutorial

### [What is Pytorch?](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)
Make sure you know how to do the following:
- How to construct empty, random, all-ones, and all-zeros tensors
- How to specify tensor datatype
- Create a tensor with the same shape as another tensor, but a different datatype
- Look up tensor operations
- Add, subtract and [multiply](https://stackoverflow.com/questions/44524901/how-to-do-product-of-matrices-in-pytorch) tensors
- Convert between Numpy arrays and tensors
- Move tensors between CPU and GPU

## [Autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)
- Useful takeaways from this tutorial:
    - `with torch.no_grad()`
    - `requires_grad` determines whether the gradient is computed.
- Don't know how important the other stuff is. Part 3 will go into how weight updates etc. is done in practice.

## [Neural Networks](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/neural_networks_tutorial.ipynb#scrollTo=pFyfk2z9r48d)
- Notice how they define the network as a class.
- Dense layers are called "Linear"
- What the network actually does is in "forward"
    - Layers that don't need weights (e.g. max_pool) aren't members of the class
    - E.g. `F.max_pool2d` is used in `forward` but not defined in `__init__`
- `.parameters()` gets weights
- Using `zero_grad` is important at the start of each training loop.
I copied the training loop from the end of the document, it's worth remembering.
```
optimizer.zero_grad() # Zero gradients
output = net(input)
loss = criterion(output, target) # Compute loss
loss.backward() # Backprop
optimizer.step() # Update step
```

## [Training a Classifier](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/cifar10_tutorial.ipynb)
- Coming soon

## CSUA account setup & Colab (5:35-6:15)
- Go to CSUA soda office
- Set up CSUA accounts
- Run Jupyter notebook on CSUA

## Universal Music Translation Network (40m)
- Read paper
- Look at GitHub
- Results from CSUA

## Seq2Seq tutorial (40m)
- https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html

## ToneNet (40m)
- https://towardsdatascience.com/tonenet-a-musical-style-transfer-c0a18903c910

> ## From the original syllabus
> 1. Week of 2/10
>    1. Lecture: Introductions, overview of sequence-to-sequence problems and the UMTN
>    2. Workshop: Pytorch tutorial (maybe [https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) though I didn't like it as much. Might want to make some custom tutorial stuff).
>    3. Homework: None
>2. Week of 2/17
>    1. Lecture
>        1. Style transfer basics. VGG and CycleGAN style transfer on images.
>        2. Seq2Seq network and use in style transfer on text and images.
>    2. Workshop
>        1. Field trip to CSUA to get accounts.
>        2. How to log in to CSUA and run Jupyter notebooks.
>        3. Seq2Seq tutorial [https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)
>    3. Homework: None
>3. Week of 2/24
>    1. Reimplementing ToneNet (seq2seq for MIDI style transfer).
>    2. Working with raw audio: [https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html](https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html) (torchaudio tutorial, will need to look over >and see how useful this is beforehand though)
>    3. Homework: None